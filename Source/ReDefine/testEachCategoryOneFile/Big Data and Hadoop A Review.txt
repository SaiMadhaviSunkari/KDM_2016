no title

Big Data and Hadoop: A Review

Mikin K. Dagli, Brijesh B. Mehta

C. U. Shah College of Engineering and Technology, Wadhwan City

{ mikin.dagli@gmail.com, brijeshbmehta@gmail.com }

Abstract --- Big data is a term for massive data sets, a large amount of data available in complex structured or no structure form. These vast amounts of data are generated by social media and networks, scientific instruments, mobile devices, sensor technology and networks. The process of research into massive amounts of data to reveal hidden patterns and secret correlations named as big data analytics. These useful information for companies or organizations with the help of gaining richer and deeper decision in the favor of companies betterments and getting advantage over the competition. Ability to manage, analyze, summarize, visualize, and discover knowledge from the collected unstructured data in a timely manner and in a scalable fashion is very difficult task using traditional data mining tools. To analyze the data Apache introduce a new technology called hadoop.

Hadoop is open source implementation of MapReduce framework, which processes the vast amount of data in parallel on large clusters. Hadoop is combination of MapReduce and Hadoop Distributed File System (HDFS). In this paper we are going to see an overview of big data, significance of big data, how hadoop works and different flavors of hadoop.

Keywords --- Big data, Volume, Variety, Velocity, Hadoop, MapReduce.

I. INTRODUCTION

Big data and its analysis are at the center of modern science and business. These data are generated from online transactions, emails, videos, audios, images, click streams, logs, posts, search queries, health records, social networking interactions, science data, sensors and mobile phones and their applications [4,13]. They are stored in databases grow massively and become difficult to capture, form, store, manage, share, analyze and visualize via typical database software tools.

5 Exabyte (1018 bytes) of data were created by human until 2003. Today this amount of information is created in two days. In 2012, digital world of data was expanded to 2.72 zettabytes (1021 bytes). It is predicted to double every two years, reaching about 8 zettabytes of data by 2015 [8]. IBM indicates that every day 2.5 exabytes of data created also 90% of the data produced in last two years [17]. A personal computer holds about 500 gigabytes (109 bytes), so it would require about 20 billion PCs to store all of the world's data. In the past, human genome decryption process takes approximately 10 years, now not more than a week [22]. Multimedia data have big weight on internet backbone traffic and is expected to increase 70% by 2013[9]. Only Google has got more than one million servers around the worlds.

There have been 6 billion mobile subscriptions in the world and every day 10 billion text messages are sent. By the year 2020, 50 billion devices will be connected to networks and the internet [3].

In 2012, The Human Face of Big Data accomplished as a global project, which is centering in real time collect, visualize and analyze large amounts of data. According to this media project many statistics are derived. Facebook has 955 million monthly active accounts using 70 languages, 140 billion photos uploaded, 125 billion friend connections, every day 30 billion pieces of content and 2.7 billion likes and comments have been posted. Every minute, 48 hours of video are uploaded and every day, 4 billion views performed on YouTube. Google support many services as both monitories 7.2 billion pages per day and processes 20 petabytes (1015 bytes) of data daily also translates into 66 languages. 1 billion Tweets every 72 hours from more than 140 million active users on Twitter. 571 new websites are created every minute of the day [23].Within the next decade, number of information will increase by 50 times however number of information technology specialists who keep up with all that data will increase by 1.5 times. [5].

II. ISSUES IN BIG DATA Important issues have been reviewed and discussed in this

section. We can describe the characteristics of big data using three Vs, also issues in big data.

Big data requires a revolutionary step forward from traditional data analysis, characterized by its three main components through which it is formed: variety, velocity and volume as shown in Figure 1[3, 8, 13, and 17].

Figure-1 the three Vs of Big data

1). Variety: Variety makes big data really big. Big data comes from a great variety of sources and generally has in three types: structured, semi structured and unstructured. Structured data inserts a data warehouse already tagged and easily sorted but unstructured data is random and difficult to analyze. Semi-structured data does not conform to fixed fields but contains tags to separate data elements [4, 17].

2). Volume: Volume or the size of data now is larger than terabytes and petabytes. The grand scale and rise of data outstrips traditional store and analysis techniques [4, 16].

3). Velocity : Velocity is required not only for big data, but also all processes. For time limited processes, big data should be used as it streams into the organization in order to maximize its value [4,16].

There are some issues, other than these three main issues

are as following:

Storage and Transport Issues:

The quantity of data is so large that to store the data we required a new storage media that can store hundreds of petabytes of data. Current disk technology limits are about 4 terabytes per disk. So, 1 exabyte would require 25,000 disks. Even if an exabyte of data could be processed on a single computer system, it would be unable to directly attach the requisite number of disks. And for communication over network we required high bandwidth infrastructure to transfer of data.

To solution are suggested: First, process the data in place

and transmit only the resulting information. Or bring the code to the data rather then bring the data to the code. Second is prioritized the data and transmit only that data which is important for processing.

Management Issues:

The sources of this data are varied - both temporally and

spatially, by format, and by method of collection. Individuals contribute digital data in mediums comfortable to them: documents, drawings, pictures, sound and video recordings, models, software behaviors, user interface designs, etc - with or without adequate metadata describing what, when, where, who, why and how it was collected and its provenance. Yet, all this data is readily available for inspection, analysis and processing.

Processing issues:

Assume that an exabyte of data needs to be processed in its entirety. For simplicity, assume the data is chunked into blocks of 8 words, so 1 exabyte = 1K petabytes. Assuming a processor expends 100 instructions on one block at 5 gigahertz, the time required for end-to-end processing would be 20 nanoseconds. To process 1K petabytes would require a total end-to-end processing time of roughly 635 years. Thus, effective processing of exabytes of data will require extensive parallel processing and new analytics algorithms in order to provide timely and actionable information.

III. METHOD Till now we have discussed what is big data and issues in

Big data. But what technology is there for big data. The answer is Hadoop.

Apache Hadoop [1] is an open source software project that enables the distributed processing of large data sets across clusters of commodity hardware. It is designed to scale up from a single computer machine to thousands of machines, with a very high degree of fault tolerance. Rather than relying on high-end hardware, the resiliency of these clusters comes from the software's ability to detect and handle failures at the application layer.

The Apache Hadoop framework is composed of the following modules:

Hadoop Common - contains libraries and utilities needed by other Hadoop modules Hadoop Distributed File System (HDFS) - a distributed file-system that stores data on the commodity machines, providing very high aggregate bandwidth across the cluster. Hadoop MapReduce - a programming model for large scale data processing.

A. Hadoop Distributed File System (HDFS)

Hadoop Distributed File System is extended version of the Google's Google File System (GFS). The wirk of HDFS is responsible for storing the data on cluster of machines. The Hadoop runtime system coupled with HDFS manages the details of parallelism and concurrency to provide ease of parallel programming with reinforced reliability. In hadoop cluster, a master node controls a group of slave nodes on which the Map and Reduce functions run in parallel. The master node assigns a task to a slave node that has any empty task slot. There is single master node and multiple slave nodes possible in HDFS. Master node contains meta information of file. HDFS divide the data into 64MB block and divide among the nodes in cluster.

Figure-2 HDFS Architecture[26]

Typically, computing storage node and processing node in a Hadoop cluster are identical from the hardware's point of view. In other words, set of homogeneous nodes manages processing as well as storing the data. HDFS operates on top of native UNIX file system. HDFS provides replicated storage for data using cheap commodity hardware. By default replication factor is 3. HDFS is read only, random writes are not allowed. It will give you best performance when small numbers of large file are there to process.

B. MapReduce

The MapReduce programming model was proposed by

Google to support data-intensive applications running on parallel computers like commodity clusters. Two important functional programming primitives in MapReduce are Map and Reduce. The Map function is applied on application specific input data to generate a list of intermediate < key, value > pairs. Then, the Reduce function is applied to the set of intermediate pairs with the same key. Typically, the Reduce function produces zero or more output pairs by performing a merging operation. All the output pairs are finally sorted based on their key values. Programmers only need to implement the Map and Reduce functions, because a MapReduce programming framework can facilitate some operations (e.g., grouping and sorting) on a set of < key, value > pairs.

The work of MapReduce is to process the data through data node. Data node is communicating with the name node constantly through hart beat message. At every particular time interval data node will send the heart beat specifying that, that data node is alive and running properly. If any data node fails to respond at particular time then name node will identify that data node as dead, and reschedule the job of that data node to elsewhere. After finishing the all map task their intermediate result stored at local storage area, and then after reduce task start their work.

The beauty of the MapReduce model lies in its simplicity, because the programmers just have to focus on data processing functionality rather than on parallelism details. The programmers provide high-level parallelism information, thereby allowing the Map and Reduce functions to be executed in parallel across multiple nodes.

Figure-3 MapReduce Example[25]

In the past few years, the MapReduce framework has been employed to develop a wide variety of data-intensive applications (e.g., data mining and bioinformatics) in large scale systems. There exist several implementations of MapReduce on various hardware platforms. For example, Phoenix is a MapReduce implementation on multi-core processors [15]. Mars is an efficient implementation of the MapReduce model on graphics processors or GPUs [2]. Mpa-Reduce-Merge is a MapReduce implementation for relational databases [10].

IV. RELATED WORK A. Implementation of MapReduce: Some research has been

directed at implementing and evaluating performance of the MapReduce model [2][15][7][11]. For example, Ranger implemented MapReduce for shared-memory systems [15]. Phoenix leads to scalable performance for both multi-core chips and conventional symmetric multiprocessors. Bingsheng et al. developed Mars - a MapReduce framework for graphics processors (GPUs) [2]. The goal of Mars is to hide the programming complexity of GPUs behind the simple MapReduce interface.

B. MapReduce Frameworks in Heterogeneous Environments: Increasing evidence shows that heterogeneity problems must be tackled in MapReduce frameworks [14][18]. Zaharia et al. implemented a new scheduler - LATE - in Hadoop to improve MapReduce performance by speculatively executing tasks that hurt response time the most [18]. Asymmetric multi-core processors (AMPs) address the I/O bottleneck issue, using double-buffering and asynchronous I/O to support MapReduce functions in clusters with asymmetric components [14]. Chao et al. classified MapReduce workloads into three categories based on CPU and I/O utilization [6]. They designed the Triple-Queue Scheduler in light of the dynamic MapReduce workload prediction mechanism called MR-Predict. Although the above techniques can improve MapReduce performance of heterogeneous clusters, they do not take into account data locality

C. Parallel File Systems: There are two types of file systems handling large files for clusters, namely, parallel file systems and Internet service file systems [18]. Representative and data movement overhead parallel file systems in clusters are Lustre [19] and PVFS (Parallel Virtual File System) [20]. Hadoop distribution file system (HDFS) [22] is a popular Internet service file system that provides the right abstraction for data processing in MapReduce frameworks.

V. OTHER PROJECTS IN HADOOP

Other then MapReduce and HDFS, the entire Apache

Hadoop platform is now commonly considered to consist of a number of related projects.

A. Project related to Data Services[24]

HBase: Apache HBase is a non-relational (NoSQL) database that runs on top of the Hadoop Distributed File System (HDFS). It is columnar and provides fault-tolerant storage and quick access to large quantities of sparse data. It also adds transactional capabilities to Hadoop, allowing users to conduct updates, inserts and deletes. HBase provides random, real time access to your big data.

Hive: Apache Hive is data warehouse infrastructure built on top of Apache Hadoop for providing data summarization, ad-hoc query, and analysis of large datasets. It provides a mechanism to project structure onto the data in Hadoop and to query that data using a SQL-like language called HiveQL (HQL). Hive eases integration between Hadoop and tools for business intelligence and visualization.

Pig: Apache Pig allows you to write complex MapReduce transformations using a simple scripting language. Pig Latin (the language) defines a set of transformations on a data set such as aggregate, join and sort. Pig translates the Pig Latin script into MapReduce so that it can be executed within Hadoop. Pig Latin is sometimes extended using UDFs (User Defined Functions), which the user can write in Java or a scripting language and then call directly from the Pig Latin.

Sqoop: Apache Sqoop is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases. Sqoop imports data from external structured datastores into HDFS or related systems like Hive and HBase. Sqoop can also be used to extract data from Hadoop and export it to external structured datastores such as relational databases and enterprise data warehouses. Sqoop works with relational databases such as: Teradata, Netezza, Oracle, MySQL, Postgres, and HSQLDB.

Mahout: Apach Mahout is a library of scalable machine-learning algorithms, implemented on top of Apache Hadoop and using the MapReduce paradigm. Machine learning is a discipline of artificial intelligence focused on enabling machines to learn without being explicitly programmed, and it is commonly used to improve future performance based on previous outcomes. Once big data is stored on the Hadoop Distributed File System (HDFS), Mahout provides the data science tools to automatically find meaningful patterns in those big data sets. The Apache Mahout project aims to make it faster and easier to turn big data into big information.

B. Project related to Operation Services[24]

Ambari: Apache Ambari is a 100-percent open source operational framework for provisioning, managing and monitoring Apache Hadoop clusters. Ambari includes an intuitive collection of operator tools and a robust set of APIs that hide the complexity of Hadoop, simplifying the operation of clusters.

Oozie: Apach Oozie is a Java Web application used to schedule Apache Hadoop jobs. Oozie combines multiple jobs sequentially into one logical unit of work. It is integrated with the Hadoop stack and supports Hadoop jobs for Apache MapReduce, Apache Pig, Apache Hive, and Apache Sqoop. It can also be used to schedule jobs specific to a system, like Java programs or shell scripts.

ZooKeeper: Apache ZooKeeper provides operational services for a Hadoop cluster. ZooKeeper provides a distributed configuration service, a synchronization service and a naming registry for distributed systems. Distributed applications use Zookeeper to store and mediate updates to important configuration information.

VI. CONCLUSION

In this paper, we have seen an overview of big data,

significance of big data, advantage, its characteristic, some issues regarding to big data and technology which is used to implement big data concept. Apache Hadoop is used to process the big data and other related projects of Hadoop. MapReduce programming model has been successfully used at Google for many different purposes. Success of MapReduce is based on, First; the model is easy to use, even for programmers without experience with parallel and distributed systems, since it hides the details of parallelization, fault-tolerance, locality optimization, and load balancing. Second, a large variety of problems are easily expressible as MapReduce computations. Third, MapReduce can implement on large clusters of commodity hardware. It will maximum utilize the resources.

Although this paper clearly has not resolved the entire subject about this substantial topic, hopefully it has provided some useful discussion and a framework for researchers.

REFERENCES

[1]. http://lucene.apache.org/hadoop, last access 10.11.2013. [2]. B.He, W.Fang, Q.Luo, N.Govindaraju, and T.Wang. Mars: a

MapReduce framework on graphics processors. ACM, 2008. [3]. B.Gerhardt, K. Griffin and R. Klemann, "Unlocking Value in

the Fragmented World of Big Data Analytics", Cisco Internet Business Solutions Group, June 2012, http://www.cisco.com/web/about/ac79/docs/sp/InformationInfome diaries.pdf.

[4]. C. Eaton, D. Deroos, T. Deutsch, G. Lapis and P.C. Zikopoulos, Understanding Big Data: Analytics for Enterprise Class Hadoop and Streaming Data, Mc Graw-Hill Companies, 978-0-07-179053-6, 2012.

[5]. C. Tankard, "Big Data Security", Network Security Newsletter, Elsevier, ISSN 1353-4858, July 2012.

[6]. T.Chao, H.Zhou, Y.He, and L.Zha. A Dynamic MapReduce Scheduler for Heterogeneous Workloads. IEEE Computer Society, 2009.

[7]. J. Dean and S. Ghemawat. Mapreduce: Simplified data processing on large clusters. OSDI '04, pages 137-150, 2008.

[8]. Intel IT Center, "Planning Guide: Getting Started with Hadoop", Steps IT Managers Can Take to Move Forward with Big Data Analytics, June 2012. http://www.intel.com/content/dam/www/public/us/en/documents/g uides/getting-started-with-hadoop-planning-guide.pdf.

[9]. J. Manyika, M. Chui, B. Brown, J. Bughin, R. Dobbs, C. Roxburgh and A.H. Byers, "Big data: The next frontier for innovation, competition, and productivity", McKinsey Global Institute, 2011, http://www.mckinsey.com/~/media/McKinsey/dotcom/Insights %20and%20pubs/MGI/Research/Technology%20and %20Innovation/Big%20Data/MGI_big_data_full_report.ashx.

[10]. H.Yang, A.Dasdan, R.Hsiao, and D.S.Parker. Map-reducemerge: simplified relational data processing on large clusters. In SIGMOD '07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data, pages 1029-1040. ACM, 2007.

[11]. M.Isard, M.Budiu, Y.Yu, A.Birrell, and D.Fetterly. Dryad: distributed data-parallel programs from sequential building blocks. In EuroSys '07: Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference on Computer Systems 2007, pages 59-72. ACM, 2007.

[12]. http://hadoop.apache.org/, last access 01.12.2013 [13]. R.D. Schneider, Hadoop for Dummies Special Edition, John

Wiley&Sons Canada, 978-1-118-25051-8, 2012. [14]. M.Rafique, B.Rose, A.Butt, and D.Nikolopoulos. Supporting

mapreduce on large-scale asymmetric multi-core clusters. SIGOPS Oper. Syst. Rev., 43(2):25-34, 2009.

[15]. C. Ranger, R. Raghuraman, A. Penmetsa, G. Bradski, and C. Kozyrakis. Evaluating mapreduce for multi-core and multiprocessor systems. High-Performance Computer Architecture, International Symposium on, 0:13-24, 2007.

[16]. S. Madden, "From Databases to Big Data", IEEE Internet Computing, v.16, pp.4-6, June 2012.

[17]. S. Singh and N. Singh, "Big Data Analytics", 2012 International Conference on Communication, Information & Computing Technology Mumbai India, IEEE, October 2011.

[18]. M.Zaharia, A.Konwinski, A.Joseph, Y.zatz, and I.Stoica. Improving mapreduce performance in heterogeneous environments. In OSDI'08: 8th USENIX Symposium on Operating Systems Design and Implementation, October 2008.

[19]. A scalable, high performance file system. http://lustre.org, last access 10.11.2013.

[20]. Parallel virtual file system, version 2. http://www.pvfs2.org, last access 10.11.2013.

[21]. D.Borthakur. The Hadoop Distributed File System: Architecture and Design. The Apache Software Foundation, 2007.

[22]. http://hpccsystems.com/, last access 01.12.2013. [23]. http://www.humanfaceofbigdata.com/, last access 01.12.2013. [24]. http://hortonworks.com/hadoop/, last access 01.12.2013. [25]. Sarika Patel, Shyam Deshmukh, "Survey on Task Assignment

Techniques in Hadoop", International Journal of Computer Application (0975-8887), volume 59-No.14, December 2012.

[26]. https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html, last access 01.12.2013